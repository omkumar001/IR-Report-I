{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Block Sort Based Indexer and Building Term-Document Matrix (TF-IDF) on Hadoop_Experiment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX3rVapLzRSL"
      },
      "source": [
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import heapq\n",
        "import json\n",
        "import gc\n",
        "porter = PorterStemmer()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV0bz-2HzoEk",
        "outputId": "091e7797-5623-4204-c603-a0399e23d442"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOT5C1B3zriL"
      },
      "source": [
        "def remove_symbols(line):\n",
        "    return re.sub('[^A-Za-z0-9\\s]+', '', line).lower()\n",
        "\n",
        "\"\"\"\n",
        "we are using this to change list into set while dumping json into file\n",
        "\"\"\"\n",
        "class SetEncoder(json.JSONEncoder):\n",
        "  def default(self, obj):\n",
        "    if isinstance(obj, set):\n",
        "      return list(obj)\n",
        "    return json.JSONEncoder.default(self, obj)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUpOSZtszseT",
        "outputId": "04b68f61-55cb-4fdf-cda6-48606c5a50d8"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tWYosMxzwaD",
        "outputId": "141ee11c-aaf9-4aa4-dd93-364b943adac0"
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "csv.field_size_limit(sys.maxsize) # if we don't do this, we won't be able to read whole line ( try to comment this line for action )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9223372036854775807"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck5CWmNNG3mV",
        "outputId": "534fd77b-55b2-4d10-8fab-9f50066a4a8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOtNLL4szyok",
        "outputId": "ff7bc334-1d2e-4ebd-b0e9-f723fdff14e0"
      },
      "source": [
        "# going to our folder\n",
        "% cd /content/drive/MyDrive/\n",
        "# % rm -rf OP*"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho7kbX6Kz00D",
        "outputId": "c3ce4e61-7a37-4ee7-a3fb-69ebc849a877"
      },
      "source": [
        " % ls # just to confirm and see if we have file\n",
        "#  gutenberg_data.csv => has 15331 LINES\n",
        "# BLOCK_SIZE = 10000000\n",
        "BLOCK_SIZE = 100000 # 1 time around 18 secs, TOTAL TIME TAKEN => 4h25m20s"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'1Scholarship decleration form (2).gdoc'\n",
            " 201952223_10.pdf\n",
            "'201952223_1 (1).pdf'\n",
            " 201952223_11.pdf\n",
            "'201952223_1 (2).pdf'\n",
            " 201952223_12.pdf\n",
            " 201952223_13.pdf\n",
            " 201952223_14.pdf\n",
            " 201952223_15.pdf\n",
            "'201952223 (1).pdf'\n",
            " 201952223_1.pdf\n",
            "'201952223_2 (1).pdf'\n",
            "'201952223_2 (2).pdf'\n",
            " 201952223_2.pdf\n",
            "'201952223_3 (1).pdf'\n",
            "'201952223_3 (2).pdf'\n",
            " 201952223_3.pdf\n",
            "'201952223_4 (1).pdf'\n",
            " 201952223_4.pdf\n",
            "'201952223_5 (1).pdf'\n",
            " 201952223_5.pdf\n",
            " 201952223_6.pdf\n",
            " 201952223_7.pdf\n",
            " 201952223_8.pdf\n",
            " 201952223_9.pdf\n",
            "'201952223_Assignment 3 and 4.pdf'\n",
            " 201952223_CS206_MidSem.pdf\n",
            "'201952223__CS262 Lab Assignment 4.pdf'\n",
            "'201952223 CS263 Lab 3.pdf'\n",
            "'201952223 CS263 Lab 4.pdf'\n",
            "'201952223 CS263 Lab 5.pdf'\n",
            "'201952223 CS263 Lab 6.pdf'\n",
            "'201952223 CS263 Lab_Assignment10.pdf'\n",
            "'201952223 CS263 Lab_Assignment11.pdf'\n",
            "'201952223 CS263 Lab_Assignment7.pdf'\n",
            "'201952223 CS263 Lab_Assignment8.pdf'\n",
            "'201952223 CS263 Lab_Assignment9.pdf'\n",
            "'201952223_CS268_Assignment 5 and 6.pdf'\n",
            "'201952223_CS268_Assignment 7 and 8.pdf'\n",
            "'201952223_CS268_Assignment 9 and 10.pdf'\n",
            "'201952223 EE-160 Lab 1.pdf'\n",
            "'201952223 EE160 Lab 2.pdf'\n",
            "'201952223 EE160 Lab 3.pdf'\n",
            "'201952223 EE160 Lab 4.pdf'\n",
            "'201952223 EE160 Lab 5.pdf'\n",
            "'201952223 EE160 Lab 6.pdf'\n",
            "'201952223 EE160 Lab 7.pdf'\n",
            "'201952223 EE160 Lab 8.pdf'\n",
            " 201952223_form.css\n",
            " 201952223_form.htm\n",
            " 201952223_form.js\n",
            " 201952223_HS202_MidSem.pdf\n",
            "'201952223_Lab 1  Assignment CS268.pdf'\n",
            "'201952223_Lab 2 Assignment CS268.pdf'\n",
            "'201952223  Lab Assignment 1 (CS263).pdf'\n",
            "'201952223  Lab Assignment 2 (CS263).pdf'\n",
            "'201952223_Om Kumar Thakur (1).pdf'\n",
            "'201952223_OM KUMAR THAKUR (1).pdf'\n",
            "'201952223_OM_KUMAR_THAKUR (1).pdf'\n",
            "'201952223_OM KUMAR THAKUR (2).pdf'\n",
            "'201952223_OM_KUMAR_THAKUR (2).pdf'\n",
            "'201952223_OM KUMAR THAKUR (3).pdf'\n",
            "'201952223 OM KUMAR THAKUR - Assignment 2_HS201_Lab Group B.pdf'\n",
            "'201952223_OM KUMAR THAKUR EC261 lab7.pdf'\n",
            "'201952223_OM KUMAR THAKUR EC261 Lab8.pdf'\n",
            "'201952223 OM KUMAR THAKUR - HS201 Assignment Writing (1).pdf'\n",
            "'201952223 OM KUMAR THAKUR - HS201 Assignment Writing.pdf'\n",
            "'201952223_ OM KUMAR THAKUR lab-10.pdf'\n",
            "'201952223_OM KUMAR THAKUR Lab 2.pdf'\n",
            "'201952223_OM KUMAR THAKUR Lab 6.pdf'\n",
            "'201952223_ OM KUMAR THAKUR lab-9.pdf'\n",
            "'201952223_Om Kumar Thakur.pdf'\n",
            "'201952223_OM KUMAR THAKUR.pdf'\n",
            " 201952223_OM_KUMAR_THAKUR.pdf\n",
            "'201952223 OM KUMAR THAKUR  (Proposal Writing ) HS201.docx'\n",
            "'201952223_OM KUMAR THAKUR_SC201.pdf'\n",
            " 201952223_OM_KUMAR_THAKUR_Section1.pdf\n",
            " 201952223_OM_KUMAR_THAKUR_Section2.pdf\n",
            "'201952223_OM KUMAR THAUR EC-261.pdf'\n",
            " 201952223.pdf\n",
            " 201952223_Session_III.pdf\n",
            " 201952223_Session_II.pdf\n",
            " 201952223_Session_I.pdf\n",
            "'bg padding.jpg'\n",
            "'CamScanner 04-16-2020 18.06.57.pdf'\n",
            " \u001b[0m\u001b[01;34mClassroom\u001b[0m/\n",
            "'CLIMATE CHANGE SC-201 Project (Final) (1).pptx'\n",
            "'CLIMATE CHANGE SC-201 Project (Final).pptx'\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            "'Copy of CS 201: 21 07 2020'\n",
            "'Copy of CS 201: 21 07 2020 (1)'\n",
            "'CS208_201952223_EndSem (1).pdf'\n",
            " CS208_201952223_EndSem.pdf\n",
            "'CS 208 Mini Project Report Group 18.pdf'\n",
            " CS263-Lab1.gdoc\n",
            "'Design_Project_Student_Form-2020-21- (final).pdf'\n",
            "'End Sem Exam SC_201_ Attempt review.gdoc'\n",
            "'Getting started.pdf'\n",
            "'Group 10 HS202 Assignment 1.pdf'\n",
            " gutenberg_data.csv\n",
            "'HS112 STS Btech 2019-20 attendance (1).gsheet'\n",
            "'HS112 STS Btech 2019-20 attendance (2).gsheet'\n",
            "'HS112 STS Btech 2019-20 attendance.gsheet'\n",
            " HS202_201952223_EndSem.pdf\n",
            "'HS-REPORT_Five Year plan ( Group 10 ) Assignment 2.pdf'\n",
            " III-sem-it-aut-2020-21.pdf\n",
            " IMG_20210309_212935.jpg\n",
            " \u001b[01;34mIR2_output\u001b[0m/\n",
            " \u001b[01;34mJava\u001b[0m/\n",
            "'Key concepts in Cyberculture.pdf'\n",
            "'Lab 7_19-23 Oct 2020.gdoc'\n",
            "\"OM KUMAR THAKUR_euphony_blith'20.mp4\"\n",
            "'OM KUMAR THAKUR_Resume (1).pdf'\n",
            "'OM KUMAR THAKUR_Resume.pdf'\n",
            "'OO E_05_Academic Calendar Autumn Semester 2020-21 .gdoc'\n",
            "'OO E_05_Academic Calendar Autumn Semester 2020-21 .pdf'\n",
            "\"Parent's Consent Hostel.jpeg\"\n",
            "\"parent's consent.jpeg\"\n",
            " paymenthistory.pdf\n",
            "'Time table_Autumn 2020-21_02082020.gdoc'\n",
            "'Time table_Autumn 2020-21_02082020.pdf'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled Jam.gjam'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc3vrHJaz65E",
        "outputId": "c1656c65-dabd-4155-f7dc-4a8e8660e6b3"
      },
      "source": [
        "def bsbi():\n",
        "  freq_dict = defaultdict(set)\n",
        "  with open('gutenberg_data.csv') as f:\n",
        "    next(f) # just skipping first line(column headers)\n",
        "    csv_file = csv.reader(f)\n",
        "    total_files = 0\n",
        "    i = 0\n",
        "    current_block = 0\n",
        "    for line in csv_file:\n",
        "      title, author, link, id, bookshelf, text = line\n",
        "      print(id,\" => \" ,author)\n",
        "      i += 1\n",
        "      # for testing \n",
        "      if i == 100:\n",
        "        break\n",
        "      for word in text.split():\n",
        "        word = remove_symbols(word)\n",
        "        if word and word not in stop_words:\n",
        "          word = porter.stem(word)\n",
        "          if word not in freq_dict:\n",
        "            # if word is not added before, we will increase block size too\n",
        "            current_block += 1\n",
        "          \"\"\"\n",
        "          note: __contains__ will just check if doc is already there.\n",
        "          It's not exactly needed as we are using set, but we are checking it to manage block size\n",
        "          also searching in sets is faster as they are hashed (while lists are not hashed)\n",
        "          \"\"\"\n",
        "          if not freq_dict[word].__contains__(id):\n",
        "            freq_dict[word].add(id)\n",
        "            current_block += 1\n",
        "        if current_block >= BLOCK_SIZE:\n",
        "          # LETS DO THE WRITE OPERATION\n",
        "          sorted_list = sorted(freq_dict.items(), key=lambda _: _[0]) # sorting by word_id\n",
        "          with open(f'./IR2_output/output{total_files}.txt', 'w') as  f:\n",
        "            # json.dump(freq_dict, f, cls=SetEncoder)\n",
        "            for word_id, docs in sorted_list:\n",
        "              f.write(word_id)\n",
        "              for doc_id in docs:\n",
        "                f.write(f' {doc_id}')\n",
        "              f.write('\\n')\n",
        "          current_block = 0\n",
        "          freq_dict.clear()\n",
        "          total_files += 1\n",
        "          print(i, ' rows done')\n",
        "          if total_files == 4:\n",
        "            return\n",
        "      \n",
        "    sorted_list = sorted(freq_dict.items(), key=lambda _: _[0]) # sorting by word_id\n",
        "    # this is for last values\n",
        "    # TODO:: DO IT BY FN SO NO REPEATATION OF CODE\n",
        "    if len(sorted_list) > 0:\n",
        "      with open(f'./IR2_output/output{total_files}.txt', 'w') as  f:\n",
        "        # json.dump(freq_dict, f, cls=SetEncoder)\n",
        "        for word_id, docs in sorted_list:\n",
        "          f.write(word_id)\n",
        "          for doc_id in docs:\n",
        "            f.write(f' {doc_id}')\n",
        "          f.write('\\n')\n",
        "      current_block = 0\n",
        "      freq_dict.clear()\n",
        "      total_files += 1\n",
        "bsbi()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17748  =>  William T. Hornaday\n",
            "34110  =>  A. R. Harding\n",
            "38315  =>  Édouard Cuyer\n",
            "30221  =>  Color Photography, Vol. 1, No. 1 Various\n",
            "32947  =>  A. Mueller\n",
            "34063  =>  E. N. Woodcock\n",
            "31751  =>  Frank M. Chapman\n",
            "34076  =>  \n",
            "34984  =>  Lucas, Shinn, Smallwood, and Whitney\n",
            "13249  =>  William T. Hornaday\n",
            "34501  =>  A. R. Harding\n",
            "37787  =>  W. H. Hudson\n",
            "38687  =>  De Gubernatis\n",
            "36922  =>  G. Murray Levick\n",
            "34131  =>  William S. Furneaux\n",
            "15  rows done\n",
            "30000  =>  Chester A. Reed\n",
            "37122  =>  Grinnell et al.\n",
            "27887  =>  Ernest Thompson Seton\n",
            "33044  =>  Max C. Thompson\n",
            "13325  =>  Sir James Emerson Tennent\n",
            "19550  =>  Robert Armitage Sterndale\n",
            "37009  =>  Clarence Moores Weed\n",
            "38013  =>  Frederic A. Lucas\n",
            "23  rows done\n",
            "34098  =>  Elmer Harry Kreps\n",
            "33874  =>  Edward Saunders\n",
            "215  =>  Jack London\n",
            "7766  =>  Ouida\n",
            "35450  =>  Harrison Weir\n",
            "38777  =>  Albert Payson Terhune\n",
            "30667  =>  Arthur Scott Bailey\n",
            "24350  =>  Edith Wharton\n",
            "26500  =>  Edward Jesse\n",
            "32069  =>  Helen Hunt Jackson\n",
            "32300  =>  Dinks, W. N. Hutchinson, and Edward Mayhew\n",
            "34259  =>  Oliver Hartley\n",
            "24923  =>  Charles Darwin\n",
            "32518  =>  Alfred Elwes\n",
            "34175  =>  Richard Lamb Allen\n",
            "39235  =>  Mrs. Leslie Williams\n",
            "18214  =>  Maurice Maeterlinck\n",
            "19226  =>  J. Earl Clauson\n",
            "41  rows done\n",
            "27190  =>  E. Nesbit\n",
            "3332  =>  Charles Darwin\n",
            "23433  =>  Oliver Herford\n",
            "32189  =>  Allen Christian Smith\n",
            "25568  =>  Nathaniel Southgate Shaler\n",
            "30550  =>  A. J. Dawson\n",
            "11758  =>  Esther Birdsall Darling\n",
            "39205  =>  Robert Jennings\n",
            "25887  =>  Henny Kindermann\n",
            "1887  =>  Jean-Henri Fabre\n",
            "38315  =>  Édouard Cuyer\n",
            "34984  =>  Lucas, Shinn, Smallwood, and Whitney\n",
            "13249  =>  William T. Hornaday\n",
            "54  rows done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmOTD_V-z7sr"
      },
      "source": [
        "file_names = [f'./IR2_output/output{i}.txt' for i in range(947)]\n",
        "file_pointers = [open(i) for i in file_names]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD4bMJdZ0BK8"
      },
      "source": [
        "\n",
        "\n",
        "# here we are using yeild so we are JUST READING ONE LINE at a time\n",
        "\n",
        "def decorated_file(f, key):\n",
        "  for line in f:\n",
        "    yield (key(line), line)\n",
        "\n",
        "files = map(open, file_names)\n",
        "outfile = open('./IR2_output/merged_out.txt', 'w')\n",
        "\n",
        "def key_fn(line):\n",
        "    return line.split(' ', 2)[0] # returning word_id\n",
        "\n",
        "\"\"\"\n",
        "so if there'd be 2 files for example :\n",
        "a 1 2 3\n",
        "b 3 4 5\n",
        "\n",
        "and \n",
        "\n",
        "a 6 7 8\n",
        "b 8 9 10\n",
        "\n",
        "o/p would be of the form :\n",
        "a 1 2 3\n",
        "a 6 7 8\n",
        "b 3 4 5\n",
        "b 8 9 10\n",
        "\n",
        "TIME: 6m 14s (for ~992 files of size ~650kb each)\n",
        "\"\"\"\n",
        "prev = ''\n",
        "# The simple sorting version would be like below\n",
        "for line in heapq.merge(*[decorated_file(f, key_fn) for f in files]):\n",
        "  \n",
        "  if prev != line[0]:\n",
        "    # if we have new word, make sure to add new line at first\n",
        "    outfile.write(f'\\n{line[1].strip()}')\n",
        "    prev = line[0]\n",
        "  # if we have same word yet, put a space and add other ids\n",
        "  else:\n",
        "    # line[1][len(line[0]):] => We are removing the word_id string and then writing the line\n",
        "    outfile.write(f' {line[1][len(line[0]):].strip()}')\n",
        "for i in file_pointers:\n",
        "  i.close()\n",
        "outfile.close()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRLEDSon0Ebv"
      },
      "source": [
        "for i in file_pointers:\n",
        "  i.close()\n",
        "outfile.close()"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}